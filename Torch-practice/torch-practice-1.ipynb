{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[2.1945, 2.0771],\n        [2.1152, 2.0848],\n        [2.5610, 2.0509]])"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "x = torch.rand(3, 2)\n",
    "y = torch.ones(3, 2, )\n",
    "\n",
    "z = x + 2 * y\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.5610, 2.0509])\n",
      "tensor([2.0771, 2.0848, 2.0509])\n",
      "tensor([[2.1945, 2.0771],\n",
      "        [2.1152, 2.0848],\n",
      "        [2.5610, 2.0509]])\n"
     ]
    }
   ],
   "source": [
    "print(z[2, :])\n",
    "print(z[:, 1])\n",
    "print(z[:, :])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Converting ndarray to tensor and other operations. Tested with CPU version of torch, with GPU mb we can have different result."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "[1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "print(a)\n",
    "b = a.numpy()\n",
    "print(b)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2., 2., 2.])\n",
      "[2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "a.add_(1)\n",
    "print(a)\n",
    "print(b)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3., 3.])\n",
      "[3. 3. 3. 3. 3.]\n"
     ]
    }
   ],
   "source": [
    "a += 1\n",
    "print(a)\n",
    "print(b)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "В ячейке ниже можно видеть, как можно отвязать переменную а от изначальных данных, теперь в a и b у нас лежит разная информация."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4., 4., 4., 4., 4.])\n",
      "[3. 3. 3. 3. 3.]\n"
     ]
    }
   ],
   "source": [
    "a = a + 1\n",
    "print(a)\n",
    "print(b)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.]\n",
      "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "a = np.ones(5)\n",
    "print(a)\n",
    "b = torch.from_numpy(a)\n",
    "print(b)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2.]\n",
      "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "a += 1\n",
    "print(a)\n",
    "print(b)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Если хотим у тензора высчитывать градиент в будущем, то явно указываем это:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.3985,  0.6900,  0.6473], requires_grad=True)\n",
      "tensor([0.6015, 2.6900, 2.6473], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "y = x + 2\n",
    "print(y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9.7374, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = y * y * 2  # вектор длины 3\n",
    "z = z.mean()  # скаляр\n",
    "print(z)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8020, 3.5866, 3.5298])\n"
     ]
    }
   ],
   "source": [
    "z.backward()  # будет работать только если z это скаляр\n",
    "print(x.grad)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Чтобы можно было делать backward не только для скаляра - можно прописать ещё один вектор и передать его в аргументы"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 3.2080, 14.3465,  3.5616])\n"
     ]
    }
   ],
   "source": [
    "z = y * y * 2\n",
    "v = torch.tensor([1, 1, 0.003], dtype=torch.float32)\n",
    "z.backward(v)\n",
    "print(x.grad)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Когда берём градиенты (например в цикле), то помним, что значения начнут суммироваться"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.])\n",
      "tensor([6., 6., 6., 6.])\n",
      "tensor([9., 9., 9., 9.])\n"
     ]
    }
   ],
   "source": [
    "weights = torch.ones(4, requires_grad=True)\n",
    "\n",
    "for epochh in range(3):\n",
    "    model_output = (weights * 3).sum()\n",
    "\n",
    "    model_output.backward()\n",
    "\n",
    "    print(weights.grad)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Поэтому нужно градиенты очищать"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n"
     ]
    }
   ],
   "source": [
    "weights = torch.ones(4, requires_grad=True)\n",
    "\n",
    "for epochh in range(3):\n",
    "    model_output = (weights * 3).sum()\n",
    "\n",
    "    model_output.backward()\n",
    "\n",
    "    print(weights.grad)\n",
    "    weights.grad.zero_()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Напишем линейную регрессию через numpy:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "epoch 1: w = 1.200, loss = 30.00000000\n",
      "epoch 3: w = 1.872, loss = 0.76800019\n",
      "epoch 5: w = 1.980, loss = 0.01966083\n",
      "epoch 7: w = 1.997, loss = 0.00050331\n",
      "epoch 9: w = 1.999, loss = 0.00001288\n",
      "epoch 11: w = 2.000, loss = 0.00000033\n",
      "epoch 13: w = 2.000, loss = 0.00000001\n",
      "epoch 15: w = 2.000, loss = 0.00000000\n",
      "epoch 17: w = 2.000, loss = 0.00000000\n",
      "epoch 19: w = 2.000, loss = 0.00000000\n",
      "Prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "X = np.array([1, 2, 3, 4], dtype=np.float32)\n",
    "Y = np.array([2, 4, 6, 8], dtype=np.float32)\n",
    "\n",
    "w = 0.0\n",
    "\n",
    "\n",
    "# linear regression\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "\n",
    "# MSE\n",
    "def loss(y, y_pred):\n",
    "    return ((y_pred - y) ** 2).mean()\n",
    "\n",
    "\n",
    "#gradient\n",
    "def gradient(x, y, y_pred):\n",
    "    return np.dot(2 * x, y_pred - y).mean()\n",
    "\n",
    "\n",
    "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
    "\n",
    "# Training\n",
    "learning_rate = 0.01\n",
    "n_iters = 20\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # predict = forward pass\n",
    "    y_pred = forward(X)\n",
    "\n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "\n",
    "    # calculate gradients\n",
    "    dw = gradient(X, Y, y_pred)\n",
    "\n",
    "    # update weights\n",
    "    w -= learning_rate * dw\n",
    "\n",
    "    if epoch % 2 == 0:\n",
    "        print(f'epoch {epoch + 1}: w = {w:.3f}, loss = {l:.8f}')\n",
    "\n",
    "print(f'Prediction after training: f(5) = {forward(5):.3f}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "А теперь избавимся от отдельно прописанной функции градиента и воспользуемся torch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: w = 0.300, loss = 30.00000000\n",
      "epoch 3: w = 0.772, loss = 15.66018772\n",
      "epoch 5: w = 1.113, loss = 8.17471695\n",
      "epoch 7: w = 1.359, loss = 4.26725292\n",
      "epoch 9: w = 1.537, loss = 2.22753215\n",
      "epoch 11: w = 1.665, loss = 1.16278565\n",
      "epoch 13: w = 1.758, loss = 0.60698116\n",
      "epoch 15: w = 1.825, loss = 0.31684780\n",
      "epoch 17: w = 1.874, loss = 0.16539653\n",
      "epoch 19: w = 1.909, loss = 0.08633806\n",
      "epoch 21: w = 1.934, loss = 0.04506890\n",
      "epoch 23: w = 1.952, loss = 0.02352631\n",
      "epoch 25: w = 1.966, loss = 0.01228084\n",
      "epoch 27: w = 1.975, loss = 0.00641066\n",
      "epoch 29: w = 1.982, loss = 0.00334642\n",
      "epoch 31: w = 1.987, loss = 0.00174685\n",
      "epoch 33: w = 1.991, loss = 0.00091188\n",
      "epoch 35: w = 1.993, loss = 0.00047601\n",
      "epoch 37: w = 1.995, loss = 0.00024848\n",
      "epoch 39: w = 1.996, loss = 0.00012971\n",
      "epoch 41: w = 1.997, loss = 0.00006770\n",
      "epoch 43: w = 1.998, loss = 0.00003534\n",
      "epoch 45: w = 1.999, loss = 0.00001845\n",
      "epoch 47: w = 1.999, loss = 0.00000963\n",
      "epoch 49: w = 1.999, loss = 0.00000503\n",
      "epoch 51: w = 1.999, loss = 0.00000262\n",
      "epoch 53: w = 2.000, loss = 0.00000137\n",
      "epoch 55: w = 2.000, loss = 0.00000071\n",
      "epoch 57: w = 2.000, loss = 0.00000037\n",
      "epoch 59: w = 2.000, loss = 0.00000019\n",
      "epoch 61: w = 2.000, loss = 0.00000010\n",
      "epoch 63: w = 2.000, loss = 0.00000005\n",
      "epoch 65: w = 2.000, loss = 0.00000003\n",
      "epoch 67: w = 2.000, loss = 0.00000001\n",
      "epoch 69: w = 2.000, loss = 0.00000001\n",
      "epoch 71: w = 2.000, loss = 0.00000000\n",
      "epoch 73: w = 2.000, loss = 0.00000000\n",
      "epoch 75: w = 2.000, loss = 0.00000000\n",
      "epoch 77: w = 2.000, loss = 0.00000000\n",
      "epoch 79: w = 2.000, loss = 0.00000000\n",
      "epoch 81: w = 2.000, loss = 0.00000000\n",
      "epoch 83: w = 2.000, loss = 0.00000000\n",
      "epoch 85: w = 2.000, loss = 0.00000000\n",
      "epoch 87: w = 2.000, loss = 0.00000000\n",
      "epoch 89: w = 2.000, loss = 0.00000000\n",
      "epoch 91: w = 2.000, loss = 0.00000000\n",
      "epoch 93: w = 2.000, loss = 0.00000000\n",
      "epoch 95: w = 2.000, loss = 0.00000000\n",
      "epoch 97: w = 2.000, loss = 0.00000000\n",
      "epoch 99: w = 2.000, loss = 0.00000000\n",
      "Prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
    "Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
    "\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "\n",
    "# linear regression\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "\n",
    "# MSE\n",
    "def loss(y, y_pred):\n",
    "    return ((y_pred - y) ** 2).mean()\n",
    "\n",
    "\n",
    "# Training\n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # predict = forward pass\n",
    "    y_pred = forward(X)\n",
    "\n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "\n",
    "    # calculate gradients. That's the difference!!! gradients = backward pass\n",
    "    l.backward()\n",
    "\n",
    "    # Нам нужно обновить вес, при этом не поменяв функцию градиента, поэтому пишем так:\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "\n",
    "    w.grad.zero_()\n",
    "\n",
    "    if epoch % 2 == 0:\n",
    "        print(f'epoch {epoch + 1}: w = {w:.3f}, loss = {l:.8f}')\n",
    "\n",
    "print(f'Prediction after training: f(5) = {forward(5):.3f}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Теперь сделаем ещё и автоматизированный loss и также вставим pytorch optimizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: w = 0.300, loss = 30.00000000\n",
      "epoch 3: w = 0.772, loss = 15.66018772\n",
      "epoch 5: w = 1.113, loss = 8.17471695\n",
      "epoch 7: w = 1.359, loss = 4.26725292\n",
      "epoch 9: w = 1.537, loss = 2.22753215\n",
      "epoch 11: w = 1.665, loss = 1.16278565\n",
      "epoch 13: w = 1.758, loss = 0.60698116\n",
      "epoch 15: w = 1.825, loss = 0.31684780\n",
      "epoch 17: w = 1.874, loss = 0.16539653\n",
      "epoch 19: w = 1.909, loss = 0.08633806\n",
      "epoch 21: w = 1.934, loss = 0.04506890\n",
      "epoch 23: w = 1.952, loss = 0.02352631\n",
      "epoch 25: w = 1.966, loss = 0.01228084\n",
      "epoch 27: w = 1.975, loss = 0.00641066\n",
      "epoch 29: w = 1.982, loss = 0.00334642\n",
      "epoch 31: w = 1.987, loss = 0.00174685\n",
      "epoch 33: w = 1.991, loss = 0.00091188\n",
      "epoch 35: w = 1.993, loss = 0.00047601\n",
      "epoch 37: w = 1.995, loss = 0.00024848\n",
      "epoch 39: w = 1.996, loss = 0.00012971\n",
      "epoch 41: w = 1.997, loss = 0.00006770\n",
      "epoch 43: w = 1.998, loss = 0.00003534\n",
      "epoch 45: w = 1.999, loss = 0.00001845\n",
      "epoch 47: w = 1.999, loss = 0.00000963\n",
      "epoch 49: w = 1.999, loss = 0.00000503\n",
      "epoch 51: w = 1.999, loss = 0.00000262\n",
      "epoch 53: w = 2.000, loss = 0.00000137\n",
      "epoch 55: w = 2.000, loss = 0.00000071\n",
      "epoch 57: w = 2.000, loss = 0.00000037\n",
      "epoch 59: w = 2.000, loss = 0.00000019\n",
      "epoch 61: w = 2.000, loss = 0.00000010\n",
      "epoch 63: w = 2.000, loss = 0.00000005\n",
      "epoch 65: w = 2.000, loss = 0.00000003\n",
      "epoch 67: w = 2.000, loss = 0.00000001\n",
      "epoch 69: w = 2.000, loss = 0.00000001\n",
      "epoch 71: w = 2.000, loss = 0.00000000\n",
      "epoch 73: w = 2.000, loss = 0.00000000\n",
      "epoch 75: w = 2.000, loss = 0.00000000\n",
      "epoch 77: w = 2.000, loss = 0.00000000\n",
      "epoch 79: w = 2.000, loss = 0.00000000\n",
      "epoch 81: w = 2.000, loss = 0.00000000\n",
      "epoch 83: w = 2.000, loss = 0.00000000\n",
      "epoch 85: w = 2.000, loss = 0.00000000\n",
      "epoch 87: w = 2.000, loss = 0.00000000\n",
      "epoch 89: w = 2.000, loss = 0.00000000\n",
      "epoch 91: w = 2.000, loss = 0.00000000\n",
      "epoch 93: w = 2.000, loss = 0.00000000\n",
      "epoch 95: w = 2.000, loss = 0.00000000\n",
      "epoch 97: w = 2.000, loss = 0.00000000\n",
      "epoch 99: w = 2.000, loss = 0.00000000\n",
      "Prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
    "Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
    "\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# linear regression\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "# Training\n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "# loss больше сами не определяем!!!\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD([w], lr=learning_rate)\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # predict = forward pass\n",
    "    y_pred = forward(X)\n",
    "\n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "\n",
    "    # calculate gradients. That's the difference!!! gradients = backward pass\n",
    "    l.backward()\n",
    "\n",
    "    # сами веса больше не обновляем!!! Optimizer делает всё за нас\n",
    "    optimizer.step()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if epoch % 2 == 0:\n",
    "        print(f'epoch {epoch + 1}: w = {w:.3f}, loss = {l:.8f}')\n",
    "\n",
    "print(f'Prediction after training: f(5) = {forward(5):.3f}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Заменим forward на встроенную функцию"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: w = -0.345, loss = 46.72698975\n",
      "epoch 3: w = 0.227, loss = 22.67167282\n",
      "epoch 5: w = 0.624, loss = 11.08773613\n",
      "epoch 7: w = 0.901, loss = 5.50839233\n",
      "epoch 9: w = 1.094, loss = 2.82009029\n",
      "epoch 11: w = 1.229, loss = 1.52375674\n",
      "epoch 13: w = 1.323, loss = 0.89763534\n",
      "epoch 15: w = 1.390, loss = 0.59422266\n",
      "epoch 17: w = 1.436, loss = 0.44620743\n",
      "epoch 19: w = 1.470, loss = 0.37303400\n",
      "epoch 21: w = 1.494, loss = 0.33591712\n",
      "epoch 23: w = 1.511, loss = 0.31618312\n",
      "epoch 25: w = 1.524, loss = 0.30484039\n",
      "epoch 27: w = 1.534, loss = 0.29755986\n",
      "epoch 29: w = 1.542, loss = 0.29225692\n",
      "epoch 31: w = 1.548, loss = 0.28792739\n",
      "epoch 33: w = 1.553, loss = 0.28408769\n",
      "epoch 35: w = 1.557, loss = 0.28050479\n",
      "epoch 37: w = 1.561, loss = 0.27706641\n",
      "epoch 39: w = 1.564, loss = 0.27371776\n",
      "epoch 41: w = 1.567, loss = 0.27043256\n",
      "epoch 43: w = 1.570, loss = 0.26719803\n",
      "epoch 45: w = 1.573, loss = 0.26400745\n",
      "epoch 47: w = 1.576, loss = 0.26085749\n",
      "epoch 49: w = 1.578, loss = 0.25774643\n",
      "epoch 51: w = 1.581, loss = 0.25467288\n",
      "epoch 53: w = 1.584, loss = 0.25163639\n",
      "epoch 55: w = 1.586, loss = 0.24863642\n",
      "epoch 57: w = 1.589, loss = 0.24567203\n",
      "epoch 59: w = 1.591, loss = 0.24274307\n",
      "epoch 61: w = 1.594, loss = 0.23984914\n",
      "epoch 63: w = 1.596, loss = 0.23698965\n",
      "epoch 65: w = 1.598, loss = 0.23416433\n",
      "epoch 67: w = 1.601, loss = 0.23137258\n",
      "epoch 69: w = 1.603, loss = 0.22861430\n",
      "epoch 71: w = 1.606, loss = 0.22588865\n",
      "epoch 73: w = 1.608, loss = 0.22319569\n",
      "epoch 75: w = 1.610, loss = 0.22053474\n",
      "epoch 77: w = 1.613, loss = 0.21790546\n",
      "epoch 79: w = 1.615, loss = 0.21530773\n",
      "epoch 81: w = 1.617, loss = 0.21274087\n",
      "epoch 83: w = 1.620, loss = 0.21020445\n",
      "epoch 85: w = 1.622, loss = 0.20769852\n",
      "epoch 87: w = 1.624, loss = 0.20522235\n",
      "epoch 89: w = 1.626, loss = 0.20277578\n",
      "epoch 91: w = 1.629, loss = 0.20035818\n",
      "epoch 93: w = 1.631, loss = 0.19796960\n",
      "epoch 95: w = 1.633, loss = 0.19560936\n",
      "epoch 97: w = 1.635, loss = 0.19327733\n",
      "epoch 99: w = 1.637, loss = 0.19097316\n",
      "Prediction after training: f(5) = 9.255\n"
     ]
    }
   ],
   "source": [
    "X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)\n",
    "Y = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32)\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "X_test = torch.tensor([5], dtype=torch.float32)\n",
    "\n",
    "input_size = n_features\n",
    "output_size = n_features\n",
    "\n",
    "# forward сами больше не пишем!\n",
    "model = nn.Linear(input_size, output_size)\n",
    "\n",
    "\n",
    "# Training\n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "# loss больше сами не определяем!!!\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # predict = forward pass\n",
    "    y_pred = model(X)\n",
    "\n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "\n",
    "    # calculate gradients. That's the difference!!! gradients = backward pass\n",
    "    l.backward()\n",
    "\n",
    "    # сами веса больше не обновляем!!! Optimizer делает всё за нас\n",
    "    optimizer.step()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if epoch % 2 == 0:\n",
    "        [w, b] = model.parameters() # unpack parameters\n",
    "        print(f'epoch {epoch + 1}: w = {w[0][0].item():.3f}, loss = {l:.8f}')\n",
    "\n",
    "print(f'Prediction after training: f(5) = {model(X_test).item():.3f}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
